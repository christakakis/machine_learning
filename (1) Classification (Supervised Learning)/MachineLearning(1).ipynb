{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Panagiotis Christakakis\n"
        "'''"
      ],
      "metadata": {
        "id": "TOKGF2q3NIyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "D7kyj4E0qPz_"
      },
      "outputs": [],
      "source": [
        "###### Section(1) - Library Imports ######\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "import numpy as np\n",
        "import keras\n",
        "from pandas.core.arrays.categorical import factorize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Section(2) - Data Import & Anlysis ######\n",
        "\n",
        "#Importing Data, Column Headings\n",
        "fileName = 'Dataset2Use_Assignment1.xlsx'\n",
        "sheetName = 'Total'\n",
        "try:\n",
        "  sheetValues = pd.read_excel(fileName, sheetName)\n",
        "  print('.. successful parsing of file:', fileName)\n",
        "  print('Column Headings:')\n",
        "  print(sheetValues.columns)\n",
        "except FileNotFoundError:\n",
        "  print(FileNotFoundError)\n",
        "\n",
        "#NA values for each feature, Type of variable for each Column\n",
        "print(\"\\n\", sheetValues.isna().sum())\n",
        "print(\"\\n\", sheetValues.info(), \"\\n\")\n",
        "\n",
        "#Factorize Output Categorical Values\n",
        "inputData = sheetValues[sheetValues.columns[:-2]].values\n",
        "outputData = sheetValues[sheetValues.columns[-2]]\n",
        "outputData, levels = pd.factorize(outputData)\n",
        "\n",
        "#Paradigms, Features, Distribution of Class Labels\n",
        "print(' .. we have', inputData.shape[0], 'available paradigms.')\n",
        "print(' .. each paradigm has', inputData.shape[1], 'features')\n",
        "print(' ... the distribution for the available class lebels is:')\n",
        "for classIdx in range(0, len(np.unique(outputData))):\n",
        " tmpCount = sum(outputData == classIdx)\n",
        " tmpPercentage = tmpCount/len(outputData)\n",
        " print(' .. class', str(classIdx), 'has', str(tmpCount), 'instances', '(','{:.2f}'.format(tmpPercentage), '%)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBqDyx_DtHEh",
        "outputId": "151aed9f-391d-4998-bd79-b443e3078698"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. successful parsing of file: Dataset2Use_Assignment1.xlsx\n",
            "Column Headings:\n",
            "Index(['365* ( Β.Υ / Κοστ.Πωλ )', 'Λειτ.Αποτ/Συν.Ενεργ. (ROA)',\n",
            "       'ΧΡΗΜ.ΔΑΠΑΝΕΣ / ΠΩΛΗΣΕΙΣ',\n",
            "       ' ΠΡΑΓΜΑΤΙΚΗ ΡΕΥΣΤΟΤΗΤΑ :  (ΚΕ-ΑΠΟΘΕΜΑΤΑ) / Β.Υ', '(ΑΠΑΙΤ.*365) / ΠΩΛ.',\n",
            "       'Συν.Υποχρ/Συν.Ενεργ', 'Διάρκεια Παραμονής Αποθεμάτων',\n",
            "       'Λογαριθμος Προσωπικού', 'ΕΝΔΕΙΞΗ ΕΞΑΓΩΓΩΝ', 'ΕΝΔΕΙΞΗ ΕΙΣΑΓΩΓΩΝ',\n",
            "       'ΕΝΔΕΙΞΗ ΑΝΤΙΠΡΟΣΩΠΕΙΩΝ', 'ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)', 'ΕΤΟΣ'],\n",
            "      dtype='object')\n",
            "\n",
            " 365* ( Β.Υ / Κοστ.Πωλ )                           0\n",
            "Λειτ.Αποτ/Συν.Ενεργ. (ROA)                        0\n",
            "ΧΡΗΜ.ΔΑΠΑΝΕΣ / ΠΩΛΗΣΕΙΣ                           0\n",
            " ΠΡΑΓΜΑΤΙΚΗ ΡΕΥΣΤΟΤΗΤΑ :  (ΚΕ-ΑΠΟΘΕΜΑΤΑ) / Β.Υ    0\n",
            "(ΑΠΑΙΤ.*365) / ΠΩΛ.                               0\n",
            "Συν.Υποχρ/Συν.Ενεργ                               0\n",
            "Διάρκεια Παραμονής Αποθεμάτων                     0\n",
            "Λογαριθμος Προσωπικού                             0\n",
            "ΕΝΔΕΙΞΗ ΕΞΑΓΩΓΩΝ                                  0\n",
            "ΕΝΔΕΙΞΗ ΕΙΣΑΓΩΓΩΝ                                 0\n",
            "ΕΝΔΕΙΞΗ ΑΝΤΙΠΡΟΣΩΠΕΙΩΝ                            0\n",
            "ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)                     0\n",
            "ΕΤΟΣ                                              0\n",
            "dtype: int64\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 10716 entries, 0 to 10715\n",
            "Data columns (total 13 columns):\n",
            " #   Column                                          Non-Null Count  Dtype  \n",
            "---  ------                                          --------------  -----  \n",
            " 0   365* ( Β.Υ / Κοστ.Πωλ )                         10716 non-null  float64\n",
            " 1   Λειτ.Αποτ/Συν.Ενεργ. (ROA)                      10716 non-null  float64\n",
            " 2   ΧΡΗΜ.ΔΑΠΑΝΕΣ / ΠΩΛΗΣΕΙΣ                         10716 non-null  float64\n",
            " 3    ΠΡΑΓΜΑΤΙΚΗ ΡΕΥΣΤΟΤΗΤΑ :  (ΚΕ-ΑΠΟΘΕΜΑΤΑ) / Β.Υ  10716 non-null  float64\n",
            " 4   (ΑΠΑΙΤ.*365) / ΠΩΛ.                             10716 non-null  float64\n",
            " 5   Συν.Υποχρ/Συν.Ενεργ                             10716 non-null  float64\n",
            " 6   Διάρκεια Παραμονής Αποθεμάτων                   10716 non-null  float64\n",
            " 7   Λογαριθμος Προσωπικού                           10716 non-null  float64\n",
            " 8   ΕΝΔΕΙΞΗ ΕΞΑΓΩΓΩΝ                                10716 non-null  int64  \n",
            " 9   ΕΝΔΕΙΞΗ ΕΙΣΑΓΩΓΩΝ                               10716 non-null  int64  \n",
            " 10  ΕΝΔΕΙΞΗ ΑΝΤΙΠΡΟΣΩΠΕΙΩΝ                          10716 non-null  int64  \n",
            " 11  ΕΝΔΕΙΞΗ ΑΣΥΝΕΠΕΙΑΣ (=2) (ν+1)                   10716 non-null  int64  \n",
            " 12  ΕΤΟΣ                                            10716 non-null  int64  \n",
            "dtypes: float64(8), int64(5)\n",
            "memory usage: 1.1 MB\n",
            "\n",
            " None \n",
            "\n",
            " .. we have 10716 available paradigms.\n",
            " .. each paradigm has 11 features\n",
            " ... the distribution for the available class lebels is:\n",
            " .. class 0 has 10468 instances ( 0.98 %)\n",
            " .. class 1 has 248 instances ( 0.02 %)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Section(3) - Train_Test Split ######\n",
        "\n",
        "# Split Data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(inputData, outputData, test_size = 0.25, stratify = outputData, shuffle = True)\n",
        "\n",
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "number_of_training_samples_train = X_train.shape[0]\n",
        "non_healthy_counter_train = (y_train == 1).sum()\n",
        "print(\"Number of training samples in TRAIN: \", number_of_training_samples_train, \"and number of Non-Healthy companies in TRAIN: \", non_healthy_counter_train, \"\\n\")\n",
        "\n",
        "number_of_training_samples_test = X_test.shape[0]\n",
        "non_healthy_counter_test = (y_test == 1).sum()\n",
        "print(\"Number of training samples in TEST: \", number_of_training_samples_test, \"and number of Non-Healthy companies in TEST: \", non_healthy_counter_test, \"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HBV-EmZK0zm2",
        "outputId": "6bde0f8d-0623-4f49-9095-848ee0317584"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples in TRAIN:  8037 and number of Non-Healthy companies in TRAIN:  186 \n",
            "\n",
            "Number of training samples in TEST:  2679 and number of Non-Healthy companies in TEST:  62 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Section(4) - Model Fitting ######\n",
        "#In order to run each model put a comment with (#) in front \n",
        "#of (''') at the start and at the end of each model section\n",
        "\n",
        "\n",
        "# Section(4.1) - Linear Discriminant Analysis\n",
        "'''\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda = LinearDiscriminantAnalysis(solver = \"svd\")\n",
        "lda.fit(X_train, y_train)\n",
        "y_pred_train = lda.predict(X_train)\n",
        "y_pred_test = lda.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.2) - Logistic Regression\n",
        "'''\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(solver = \"lbfgs\")\n",
        "logreg.fit(X_train, y_train)\n",
        "y_pred_train = logreg.predict(X_train)\n",
        "y_pred_test = logreg.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.3) - Decision Trees\n",
        "'''\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(max_depth = 4)\n",
        "clf.fit(X_train, y_train)\n",
        "y_pred_train = clf.predict(X_train)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.4) - k-Nearest Neighbors\n",
        "'''\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred_train = knn.predict(X_train)\n",
        "y_pred_test = knn.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.5) - Naïve Bayes\n",
        "'''\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred_train = gnb.predict(X_train)\n",
        "y_pred_test = gnb.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.6) - Support Vector Machines\n",
        "# Hyper-parameter tuning was done after a loop\n",
        "# process with GridSearchCV of a totaling 125 fits\n",
        "'''\n",
        "from sklearn.svm import SVC\n",
        "svm = SVC(C = 0.1, gamma = 1, kernel = \"rbf\")\n",
        "svm.fit(X_train, y_train)\n",
        "y_pred_train = svm.predict(X_train)\n",
        "y_pred_test = svm.predict(X_test)\n",
        "'''\n",
        "\n",
        "\n",
        "# Section(4.7) - Neural Networks\n",
        "#The dataset we're trying to classify is imbalanced.\n",
        "#Class_0 to Class_1 ratio is 43 to 1. So even a neural\n",
        "#network this big, can't even overfit to some Class_1 \n",
        "#training samples. Training set resampling is required\n",
        "#'''\n",
        "CustomModel = keras.models.Sequential()\n",
        "CustomModel.add(keras.layers.Dense(512, input_dim = X_train.shape[1], activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(256, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(128, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(64, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(32, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(16, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(8, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(4, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(2, activation='softmax'))\n",
        "#CustomModel.summary()\n",
        "CustomModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "CustomModel.fit(X_train, keras.utils.np_utils.to_categorical(y_train), batch_size = 512, epochs=100, verbose=1)\n",
        "\n",
        "y_pred_train = np.argmax((CustomModel.predict(X_train) > 0.5).astype(\"int32\"), axis=1)\n",
        "y_pred_test = np.argmax((CustomModel.predict(X_test) > 0.5).astype(\"int32\"), axis=1)\n",
        "#'''\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "id": "BOkVslsXnwy9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e38c4b24-53b1-4ffd-b45f-80b3f3cf970b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "16/16 [==============================] - 2s 32ms/step - loss: 0.4351 - accuracy: 0.9185\n",
            "Epoch 2/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.1346 - accuracy: 0.9769\n",
            "Epoch 3/100\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.1094 - accuracy: 0.9769\n",
            "Epoch 4/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.1008 - accuracy: 0.9769\n",
            "Epoch 5/100\n",
            "16/16 [==============================] - 1s 43ms/step - loss: 0.0949 - accuracy: 0.9769\n",
            "Epoch 6/100\n",
            "16/16 [==============================] - 1s 41ms/step - loss: 0.0913 - accuracy: 0.9769\n",
            "Epoch 7/100\n",
            "16/16 [==============================] - 1s 38ms/step - loss: 0.0896 - accuracy: 0.9769\n",
            "Epoch 8/100\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0885 - accuracy: 0.9769\n",
            "Epoch 9/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0878 - accuracy: 0.9769\n",
            "Epoch 10/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0872 - accuracy: 0.9769\n",
            "Epoch 11/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0866 - accuracy: 0.9769\n",
            "Epoch 12/100\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0864 - accuracy: 0.9769\n",
            "Epoch 13/100\n",
            "16/16 [==============================] - 1s 35ms/step - loss: 0.0863 - accuracy: 0.9769\n",
            "Epoch 14/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0850 - accuracy: 0.9769\n",
            "Epoch 15/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0854 - accuracy: 0.9769\n",
            "Epoch 16/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0844 - accuracy: 0.9769\n",
            "Epoch 17/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0836 - accuracy: 0.9769\n",
            "Epoch 18/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0839 - accuracy: 0.9769\n",
            "Epoch 19/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0836 - accuracy: 0.9769\n",
            "Epoch 20/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0826 - accuracy: 0.9769\n",
            "Epoch 21/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0823 - accuracy: 0.9769\n",
            "Epoch 22/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0817 - accuracy: 0.9769\n",
            "Epoch 23/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0815 - accuracy: 0.9769\n",
            "Epoch 24/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0821 - accuracy: 0.9769\n",
            "Epoch 25/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0808 - accuracy: 0.9769\n",
            "Epoch 26/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0796 - accuracy: 0.9769\n",
            "Epoch 27/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0786 - accuracy: 0.9769\n",
            "Epoch 28/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0788 - accuracy: 0.9769\n",
            "Epoch 29/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0777 - accuracy: 0.9769\n",
            "Epoch 30/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0771 - accuracy: 0.9769\n",
            "Epoch 31/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0772 - accuracy: 0.9769\n",
            "Epoch 32/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0763 - accuracy: 0.9769\n",
            "Epoch 33/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0754 - accuracy: 0.9769\n",
            "Epoch 34/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0735 - accuracy: 0.9772\n",
            "Epoch 35/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0724 - accuracy: 0.9792\n",
            "Epoch 36/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0737 - accuracy: 0.9774\n",
            "Epoch 37/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0703 - accuracy: 0.9784\n",
            "Epoch 38/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0700 - accuracy: 0.9792\n",
            "Epoch 39/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0704 - accuracy: 0.9776\n",
            "Epoch 40/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0738 - accuracy: 0.9782\n",
            "Epoch 41/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0711 - accuracy: 0.9788\n",
            "Epoch 42/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0704 - accuracy: 0.9793\n",
            "Epoch 43/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0682 - accuracy: 0.9796\n",
            "Epoch 44/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0662 - accuracy: 0.9802\n",
            "Epoch 45/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0651 - accuracy: 0.9803\n",
            "Epoch 46/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0679 - accuracy: 0.9796\n",
            "Epoch 47/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0644 - accuracy: 0.9806\n",
            "Epoch 48/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0660 - accuracy: 0.9810\n",
            "Epoch 49/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0635 - accuracy: 0.9820\n",
            "Epoch 50/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0613 - accuracy: 0.9815\n",
            "Epoch 51/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0614 - accuracy: 0.9813\n",
            "Epoch 52/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0620 - accuracy: 0.9811\n",
            "Epoch 53/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0590 - accuracy: 0.9828\n",
            "Epoch 54/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0604 - accuracy: 0.9818\n",
            "Epoch 55/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0613 - accuracy: 0.9810\n",
            "Epoch 56/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0605 - accuracy: 0.9815\n",
            "Epoch 57/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0566 - accuracy: 0.9841\n",
            "Epoch 58/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0612 - accuracy: 0.9811\n",
            "Epoch 59/100\n",
            "16/16 [==============================] - 1s 33ms/step - loss: 0.0623 - accuracy: 0.9807\n",
            "Epoch 60/100\n",
            "16/16 [==============================] - 0s 31ms/step - loss: 0.0569 - accuracy: 0.9825\n",
            "Epoch 61/100\n",
            "16/16 [==============================] - 1s 39ms/step - loss: 0.0541 - accuracy: 0.9838\n",
            "Epoch 62/100\n",
            "16/16 [==============================] - 1s 32ms/step - loss: 0.0539 - accuracy: 0.9835\n",
            "Epoch 63/100\n",
            "16/16 [==============================] - 0s 27ms/step - loss: 0.0528 - accuracy: 0.9846\n",
            "Epoch 64/100\n",
            "16/16 [==============================] - 0s 30ms/step - loss: 0.0560 - accuracy: 0.9832\n",
            "Epoch 65/100\n",
            "16/16 [==============================] - 1s 31ms/step - loss: 0.0533 - accuracy: 0.9842\n",
            "Epoch 66/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0527 - accuracy: 0.9844\n",
            "Epoch 67/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0509 - accuracy: 0.9853\n",
            "Epoch 68/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0521 - accuracy: 0.9838\n",
            "Epoch 69/100\n",
            "16/16 [==============================] - 0s 29ms/step - loss: 0.0526 - accuracy: 0.9839\n",
            "Epoch 70/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0488 - accuracy: 0.9856\n",
            "Epoch 71/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0479 - accuracy: 0.9862\n",
            "Epoch 72/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0518 - accuracy: 0.9841\n",
            "Epoch 73/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0484 - accuracy: 0.9859\n",
            "Epoch 74/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0467 - accuracy: 0.9854\n",
            "Epoch 75/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0500 - accuracy: 0.9851\n",
            "Epoch 76/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0461 - accuracy: 0.9861\n",
            "Epoch 77/100\n",
            "16/16 [==============================] - 0s 22ms/step - loss: 0.0496 - accuracy: 0.9849\n",
            "Epoch 78/100\n",
            "16/16 [==============================] - 0s 24ms/step - loss: 0.0496 - accuracy: 0.9849\n",
            "Epoch 79/100\n",
            "16/16 [==============================] - 0s 26ms/step - loss: 0.0464 - accuracy: 0.9857\n",
            "Epoch 80/100\n",
            "16/16 [==============================] - 1s 31ms/step - loss: 0.0432 - accuracy: 0.9861\n",
            "Epoch 81/100\n",
            "16/16 [==============================] - 1s 34ms/step - loss: 0.0468 - accuracy: 0.9854\n",
            "Epoch 82/100\n",
            "16/16 [==============================] - 1s 36ms/step - loss: 0.0463 - accuracy: 0.9866\n",
            "Epoch 83/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0436 - accuracy: 0.9867\n",
            "Epoch 84/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0416 - accuracy: 0.9868\n",
            "Epoch 85/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0434 - accuracy: 0.9863\n",
            "Epoch 86/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0490 - accuracy: 0.9838\n",
            "Epoch 87/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0609 - accuracy: 0.9822\n",
            "Epoch 88/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0529 - accuracy: 0.9831\n",
            "Epoch 89/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0479 - accuracy: 0.9843\n",
            "Epoch 90/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0421 - accuracy: 0.9876\n",
            "Epoch 91/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0402 - accuracy: 0.9873\n",
            "Epoch 92/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0383 - accuracy: 0.9877\n",
            "Epoch 93/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0418 - accuracy: 0.9871\n",
            "Epoch 94/100\n",
            "16/16 [==============================] - 0s 20ms/step - loss: 0.0397 - accuracy: 0.9874\n",
            "Epoch 95/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0400 - accuracy: 0.9872\n",
            "Epoch 96/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0387 - accuracy: 0.9884\n",
            "Epoch 97/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0364 - accuracy: 0.9881\n",
            "Epoch 98/100\n",
            "16/16 [==============================] - 0s 18ms/step - loss: 0.0368 - accuracy: 0.9886\n",
            "Epoch 99/100\n",
            "16/16 [==============================] - 0s 17ms/step - loss: 0.0364 - accuracy: 0.9878\n",
            "Epoch 100/100\n",
            "16/16 [==============================] - 0s 19ms/step - loss: 0.0350 - accuracy: 0.9888\n",
            "252/252 [==============================] - 1s 2ms/step\n",
            "84/84 [==============================] - 0s 2ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.99      0.98      2617\n",
            "           1       0.17      0.10      0.12        62\n",
            "\n",
            "    accuracy                           0.97      2679\n",
            "   macro avg       0.57      0.54      0.55      2679\n",
            "weighted avg       0.96      0.97      0.96      2679\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Section(5) - Metrics for Train_Test ######\n",
        "\n",
        "# Calculate Accuracy, Precision, Recall, F1-Score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "acc_train = accuracy_score(y_train, y_pred_train)\n",
        "acc_test = accuracy_score(y_test, y_pred_test)\n",
        "pre_train = precision_score(y_train, y_pred_train, average='macro')\n",
        "pre_test = precision_score(y_test, y_pred_test, average='macro')\n",
        "rec_train = recall_score(y_train, y_pred_train, average='macro') \n",
        "rec_test = recall_score(y_test, y_pred_test, average ='macro')\n",
        "f1_train = f1_score(y_train, y_pred_train, average='macro')\n",
        "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
        "# print the scores\n",
        "print('Accuracy scores of <ModelName> classifier are:', 'train: {:.2f}'.format(acc_train), 'and test: {:.2f}.'.format(acc_test))\n",
        "print('Precision scores of <ModelName> classifier are:', 'train: {:.2f}'.format(pre_train), 'and test: {:.2f}.'.format(pre_test))\n",
        "print('Recall scores of <ModelName> classifier are:', 'train: {:.2f}'.format(rec_train), 'and test: {:.2f}.'.format(rec_test))\n",
        "print('F1 scores of <ModelName> classifier are:', 'train: {:.2f}'.format(f1_train), 'and test: {:.2f}.'.format(f1_test))\n",
        "\n",
        "# Calculate TP, TN, FP, FN\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Calculatin TP,TN,FP,FN of training set\n",
        "tp_train, fp_train, fn_train, tn_train = confusion_matrix(y_train, y_pred_train).ravel()\n",
        "print(tp_train, fp_train, fn_train, tn_train)\n",
        "\n",
        "#Calculatin TP,TN,FP,FN of test set\n",
        "tp_test, fp_test, fn_test, tn_test = confusion_matrix(y_test, y_pred_test, labels=[0, 1]).ravel()\n",
        "print(tp_test, fp_test, fn_test, tn_test)\n",
        "\n",
        "#Percentage Conditions of Models\n",
        "percantage_bankrupt = tn_test / np.count_nonzero(y_test == 1)\n",
        "percantage_non_bankrupt = tp_test / np.count_nonzero(y_test == 0)\n",
        "print(percantage_bankrupt)\n",
        "print(percantage_non_bankrupt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMuui5pdoevT",
        "outputId": "30cea45d-cde8-4d01-c678-b0da37848e8a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores of <ModelName> classifier are: train: 0.99 and test: 0.97.\n",
            "Precision scores of <ModelName> classifier are: train: 0.95 and test: 0.57.\n",
            "Recall scores of <ModelName> classifier are: train: 0.81 and test: 0.54.\n",
            "F1 scores of <ModelName> classifier are: train: 0.87 and test: 0.55.\n",
            "7839 12 69 117\n",
            "2587 30 56 6\n",
            "0.0967741935483871\n",
            "0.988536492166603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Section(6) - Training Set Resampling ######\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "#Undersampling the majority class in order to have 3:1 ratio\n",
        "rus = RandomUnderSampler(sampling_strategy=1/3)\n",
        "X_train_rus, y_train_rus = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "number_of_training_samples_train = X_train_rus.shape[0]\n",
        "non_healthy_counter_train = (y_train_rus == 1).sum()\n",
        "print(\"Number of training samples in TRAIN: \", number_of_training_samples_train, \"and number of Non-Healthy companies in TRAIN: \", non_healthy_counter_train, \"\\n\")\n",
        "\n",
        "number_of_training_samples_test = X_test.shape[0]\n",
        "non_healthy_counter_test = (y_test == 1).sum()\n",
        "print(\"Number of training samples in TEST: \", number_of_training_samples_test, \"and number of Non-Healthy companies in TEST: \", non_healthy_counter_test, \"\\n\")\n",
        "\n",
        "print(pd.value_counts(y_train_rus))"
      ],
      "metadata": {
        "id": "ZPu16rA-v2mN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840acfea-5eba-46fc-fc16-b8f9c5d7acc9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples in TRAIN:  744 and number of Non-Healthy companies in TRAIN:  186 \n",
            "\n",
            "Number of training samples in TEST:  2679 and number of Non-Healthy companies in TEST:  62 \n",
            "\n",
            "0    558\n",
            "1    186\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Section(7) - Model Fitting with Resampled Training Set ######\n",
        "#In order to run each model put a comment with (#) in front \n",
        "#of (''') at the start and at the end of each model section\n",
        "\n",
        "\n",
        "# Section(4.1) - Linear Discriminant Analysis\n",
        "'''\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "lda = LinearDiscriminantAnalysis(solver = \"svd\")\n",
        "lda.fit(X_train_rus, y_train_rus)\n",
        "y_pred_train = lda.predict(X_train_rus)\n",
        "y_pred_test = lda.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.2) - Logistic Regression\n",
        "'''\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(solver = \"lbfgs\")\n",
        "logreg.fit(X_train_rus, y_train_rus)\n",
        "y_pred_train = logreg.predict(X_train_rus)\n",
        "y_pred_test = logreg.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.3) - Decision Trees\n",
        "'''\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "clf = DecisionTreeClassifier(max_depth = 4)\n",
        "clf.fit(X_train_rus, y_train_rus)\n",
        "y_pred_train = clf.predict(X_train_rus)\n",
        "y_pred_test = clf.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.4) - k-Nearest Neighbors\n",
        "'''\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors = 3)\n",
        "knn.fit(X_train_rus, y_train_rus)\n",
        "y_pred_train = knn.predict(X_train_rus)\n",
        "y_pred_test = knn.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.5) - Naïve Bayes\n",
        "'''\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train_rus, y_train_rus)\n",
        "y_pred_train = gnb.predict(X_train_rus)\n",
        "y_pred_test = gnb.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.6) - Support Vector Machines\n",
        "'''\n",
        "from sklearn.svm import SVC\n",
        "svm = SVC()\n",
        "svm.fit(X_train_rus, y_train_rus)\n",
        "y_pred_train = svm.predict(X_train_rus)\n",
        "y_pred_test = svm.predict(X_test)\n",
        "'''\n",
        "\n",
        "# Section(4.7) - Neural Networks\n",
        "#'''\n",
        "import tensorflow as tf\n",
        "from keras.utils import np_utils\n",
        "\n",
        "custom_early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                        monitor='accuracy',\n",
        "                        min_delta=0.001,\n",
        "                        patience=100,\n",
        "                        mode='auto',\n",
        "                        )\n",
        "\n",
        "CustomModel = keras.models.Sequential()\n",
        "CustomModel.add(keras.layers.Dense(64, input_dim = X_train_rus.shape[1], activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(16, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(16, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(32, activation='relu'))\n",
        "CustomModel.add(keras.layers.Dense(2, activation='softmax'))\n",
        "#CustomModel.add(keras.layers.Dense(1, activation='sigmoid'))\n",
        "#CustomModel.summary()\n",
        "\n",
        "CustomModel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "CustomModel.fit(X_train_rus, keras.utils.np_utils.to_categorical(y_train_rus), callbacks=[custom_early_stopping],\n",
        "                batch_size = 50, epochs=200, verbose=1)\n",
        "\n",
        "y_pred_train = np.argmax((CustomModel.predict(X_train_rus) > 0.5).astype(\"int32\"), axis=1)\n",
        "y_pred_test = np.argmax((CustomModel.predict(X_test) > 0.5).astype(\"int32\"), axis =1)\n",
        "#'''\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "print(classification_report(y_test, y_pred_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xj7L_--DdnId",
        "outputId": "233a6f89-45b3-43ea-d24f-12c294fdbaca"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "15/15 [==============================] - 1s 2ms/step - loss: 0.6405 - accuracy: 0.7513\n",
            "Epoch 2/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5581 - accuracy: 0.7500\n",
            "Epoch 3/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.5206 - accuracy: 0.7500\n",
            "Epoch 4/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.5000 - accuracy: 0.7500\n",
            "Epoch 5/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4808 - accuracy: 0.7500\n",
            "Epoch 6/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4614 - accuracy: 0.7513\n",
            "Epoch 7/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4433 - accuracy: 0.7581\n",
            "Epoch 8/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.4282 - accuracy: 0.7903\n",
            "Epoch 9/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4182 - accuracy: 0.8024\n",
            "Epoch 10/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4141 - accuracy: 0.8024\n",
            "Epoch 11/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.4110 - accuracy: 0.8024\n",
            "Epoch 12/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4059 - accuracy: 0.8078\n",
            "Epoch 13/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.4016 - accuracy: 0.8118\n",
            "Epoch 14/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3996 - accuracy: 0.8185\n",
            "Epoch 15/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3962 - accuracy: 0.8118\n",
            "Epoch 16/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3964 - accuracy: 0.8118\n",
            "Epoch 17/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3959 - accuracy: 0.8172\n",
            "Epoch 18/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3890 - accuracy: 0.8239\n",
            "Epoch 19/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3863 - accuracy: 0.8239\n",
            "Epoch 20/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3856 - accuracy: 0.8226\n",
            "Epoch 21/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3828 - accuracy: 0.8239\n",
            "Epoch 22/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3796 - accuracy: 0.8333\n",
            "Epoch 23/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3797 - accuracy: 0.8266\n",
            "Epoch 24/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3880 - accuracy: 0.8159\n",
            "Epoch 25/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3865 - accuracy: 0.8199\n",
            "Epoch 26/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3763 - accuracy: 0.8293\n",
            "Epoch 27/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3702 - accuracy: 0.8306\n",
            "Epoch 28/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3646 - accuracy: 0.8360\n",
            "Epoch 29/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3652 - accuracy: 0.8360\n",
            "Epoch 30/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3675 - accuracy: 0.8333\n",
            "Epoch 31/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3604 - accuracy: 0.8374\n",
            "Epoch 32/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3572 - accuracy: 0.8387\n",
            "Epoch 33/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3536 - accuracy: 0.8347\n",
            "Epoch 34/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3561 - accuracy: 0.8253\n",
            "Epoch 35/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.3520 - accuracy: 0.8387\n",
            "Epoch 36/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3507 - accuracy: 0.8347\n",
            "Epoch 37/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3482 - accuracy: 0.8266\n",
            "Epoch 38/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3424 - accuracy: 0.8333\n",
            "Epoch 39/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3419 - accuracy: 0.8454\n",
            "Epoch 40/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3374 - accuracy: 0.8481\n",
            "Epoch 41/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3378 - accuracy: 0.8347\n",
            "Epoch 42/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3290 - accuracy: 0.8495\n",
            "Epoch 43/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3236 - accuracy: 0.8468\n",
            "Epoch 44/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3202 - accuracy: 0.8522\n",
            "Epoch 45/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3203 - accuracy: 0.8562\n",
            "Epoch 46/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3157 - accuracy: 0.8548\n",
            "Epoch 47/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3152 - accuracy: 0.8522\n",
            "Epoch 48/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.3100 - accuracy: 0.8575\n",
            "Epoch 49/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.3050 - accuracy: 0.8656\n",
            "Epoch 50/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2993 - accuracy: 0.8602\n",
            "Epoch 51/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2927 - accuracy: 0.8629\n",
            "Epoch 52/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2998 - accuracy: 0.8642\n",
            "Epoch 53/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2892 - accuracy: 0.8710\n",
            "Epoch 54/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2860 - accuracy: 0.8696\n",
            "Epoch 55/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2845 - accuracy: 0.8804\n",
            "Epoch 56/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2844 - accuracy: 0.8777\n",
            "Epoch 57/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2738 - accuracy: 0.8992\n",
            "Epoch 58/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2760 - accuracy: 0.8911\n",
            "Epoch 59/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2887 - accuracy: 0.8669\n",
            "Epoch 60/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2736 - accuracy: 0.8790\n",
            "Epoch 61/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2614 - accuracy: 0.8925\n",
            "Epoch 62/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2620 - accuracy: 0.8858\n",
            "Epoch 63/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2567 - accuracy: 0.8952\n",
            "Epoch 64/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2538 - accuracy: 0.8978\n",
            "Epoch 65/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2493 - accuracy: 0.8992\n",
            "Epoch 66/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2506 - accuracy: 0.9073\n",
            "Epoch 67/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2545 - accuracy: 0.8925\n",
            "Epoch 68/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2381 - accuracy: 0.9059\n",
            "Epoch 69/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2397 - accuracy: 0.8992\n",
            "Epoch 70/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2335 - accuracy: 0.9005\n",
            "Epoch 71/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2338 - accuracy: 0.9032\n",
            "Epoch 72/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2353 - accuracy: 0.9073\n",
            "Epoch 73/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2360 - accuracy: 0.8992\n",
            "Epoch 74/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2329 - accuracy: 0.9046\n",
            "Epoch 75/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2287 - accuracy: 0.9059\n",
            "Epoch 76/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2232 - accuracy: 0.9126\n",
            "Epoch 77/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2292 - accuracy: 0.9059\n",
            "Epoch 78/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2230 - accuracy: 0.9099\n",
            "Epoch 79/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2296 - accuracy: 0.9059\n",
            "Epoch 80/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2212 - accuracy: 0.9126\n",
            "Epoch 81/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2149 - accuracy: 0.9073\n",
            "Epoch 82/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2176 - accuracy: 0.9073\n",
            "Epoch 83/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9099\n",
            "Epoch 84/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9247\n",
            "Epoch 85/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2095 - accuracy: 0.9113\n",
            "Epoch 86/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.2041 - accuracy: 0.9167\n",
            "Epoch 87/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1969 - accuracy: 0.9140\n",
            "Epoch 88/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.2069 - accuracy: 0.9194\n",
            "Epoch 89/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1979 - accuracy: 0.9234\n",
            "Epoch 90/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1896 - accuracy: 0.9247\n",
            "Epoch 91/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1908 - accuracy: 0.9167\n",
            "Epoch 92/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1905 - accuracy: 0.9207\n",
            "Epoch 93/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1845 - accuracy: 0.9220\n",
            "Epoch 94/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1813 - accuracy: 0.9207\n",
            "Epoch 95/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1948 - accuracy: 0.9194\n",
            "Epoch 96/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1956 - accuracy: 0.9140\n",
            "Epoch 97/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1778 - accuracy: 0.9261\n",
            "Epoch 98/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1814 - accuracy: 0.9288\n",
            "Epoch 99/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1889 - accuracy: 0.9247\n",
            "Epoch 100/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1854 - accuracy: 0.9194\n",
            "Epoch 101/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1739 - accuracy: 0.9341\n",
            "Epoch 102/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1817 - accuracy: 0.9288\n",
            "Epoch 103/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1713 - accuracy: 0.9288\n",
            "Epoch 104/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9341\n",
            "Epoch 105/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1710 - accuracy: 0.9261\n",
            "Epoch 106/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1671 - accuracy: 0.9341\n",
            "Epoch 107/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1731 - accuracy: 0.9167\n",
            "Epoch 108/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1790 - accuracy: 0.9207\n",
            "Epoch 109/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1711 - accuracy: 0.9409\n",
            "Epoch 110/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1572 - accuracy: 0.9355\n",
            "Epoch 111/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1478 - accuracy: 0.9368\n",
            "Epoch 112/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1696 - accuracy: 0.9395\n",
            "Epoch 113/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1569 - accuracy: 0.9422\n",
            "Epoch 114/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1508 - accuracy: 0.9382\n",
            "Epoch 115/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1888 - accuracy: 0.9247\n",
            "Epoch 116/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1698 - accuracy: 0.9355\n",
            "Epoch 117/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1666 - accuracy: 0.9368\n",
            "Epoch 118/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1514 - accuracy: 0.9382\n",
            "Epoch 119/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1457 - accuracy: 0.9382\n",
            "Epoch 120/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1392 - accuracy: 0.9530\n",
            "Epoch 121/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1399 - accuracy: 0.9516\n",
            "Epoch 122/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1367 - accuracy: 0.9449\n",
            "Epoch 123/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1400 - accuracy: 0.9476\n",
            "Epoch 124/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1519 - accuracy: 0.9435\n",
            "Epoch 125/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1352 - accuracy: 0.9462\n",
            "Epoch 126/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9583\n",
            "Epoch 127/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1232 - accuracy: 0.9570\n",
            "Epoch 128/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1294 - accuracy: 0.9489\n",
            "Epoch 129/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9570\n",
            "Epoch 130/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1208 - accuracy: 0.9583\n",
            "Epoch 131/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1225 - accuracy: 0.9597\n",
            "Epoch 132/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1239 - accuracy: 0.9543\n",
            "Epoch 133/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1197 - accuracy: 0.9597\n",
            "Epoch 134/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1432 - accuracy: 0.9449\n",
            "Epoch 135/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1233 - accuracy: 0.9489\n",
            "Epoch 136/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1185 - accuracy: 0.9543\n",
            "Epoch 137/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1153 - accuracy: 0.9597\n",
            "Epoch 138/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1116 - accuracy: 0.9624\n",
            "Epoch 139/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1113 - accuracy: 0.9624\n",
            "Epoch 140/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1170 - accuracy: 0.9624\n",
            "Epoch 141/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1146 - accuracy: 0.9570\n",
            "Epoch 142/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1071 - accuracy: 0.9664\n",
            "Epoch 143/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1187 - accuracy: 0.9503\n",
            "Epoch 144/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1068 - accuracy: 0.9583\n",
            "Epoch 145/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1114 - accuracy: 0.9543\n",
            "Epoch 146/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1235 - accuracy: 0.9530\n",
            "Epoch 147/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1084 - accuracy: 0.9651\n",
            "Epoch 148/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1128 - accuracy: 0.9624\n",
            "Epoch 149/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1142 - accuracy: 0.9570\n",
            "Epoch 150/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0950 - accuracy: 0.9691\n",
            "Epoch 151/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1026 - accuracy: 0.9651\n",
            "Epoch 152/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0965 - accuracy: 0.9691\n",
            "Epoch 153/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0986 - accuracy: 0.9637\n",
            "Epoch 154/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0964 - accuracy: 0.9677\n",
            "Epoch 155/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1203 - accuracy: 0.9583\n",
            "Epoch 156/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.1240 - accuracy: 0.9462\n",
            "Epoch 157/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1200 - accuracy: 0.9570\n",
            "Epoch 158/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1284 - accuracy: 0.9489\n",
            "Epoch 159/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9664\n",
            "Epoch 160/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0994 - accuracy: 0.9651\n",
            "Epoch 161/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0990 - accuracy: 0.9597\n",
            "Epoch 162/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0870 - accuracy: 0.9677\n",
            "Epoch 163/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0761 - accuracy: 0.9745\n",
            "Epoch 164/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0754 - accuracy: 0.9758\n",
            "Epoch 165/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0780 - accuracy: 0.9785\n",
            "Epoch 166/200\n",
            "15/15 [==============================] - 0s 6ms/step - loss: 0.0783 - accuracy: 0.9745\n",
            "Epoch 167/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0748 - accuracy: 0.9772\n",
            "Epoch 168/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9718\n",
            "Epoch 169/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0929 - accuracy: 0.9583\n",
            "Epoch 170/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0852 - accuracy: 0.9677\n",
            "Epoch 171/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0842 - accuracy: 0.9731\n",
            "Epoch 172/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0886 - accuracy: 0.9718\n",
            "Epoch 173/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0778 - accuracy: 0.9731\n",
            "Epoch 174/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0693 - accuracy: 0.9758\n",
            "Epoch 175/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0700 - accuracy: 0.9798\n",
            "Epoch 176/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0733 - accuracy: 0.9745\n",
            "Epoch 177/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0665 - accuracy: 0.9812\n",
            "Epoch 178/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0672 - accuracy: 0.9772\n",
            "Epoch 179/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0684 - accuracy: 0.9798\n",
            "Epoch 180/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0701 - accuracy: 0.9745\n",
            "Epoch 181/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0716 - accuracy: 0.9772\n",
            "Epoch 182/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0691 - accuracy: 0.9745\n",
            "Epoch 183/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0672 - accuracy: 0.9785\n",
            "Epoch 184/200\n",
            "15/15 [==============================] - 0s 5ms/step - loss: 0.0677 - accuracy: 0.9772\n",
            "Epoch 185/200\n",
            "15/15 [==============================] - 0s 4ms/step - loss: 0.0871 - accuracy: 0.9637\n",
            "Epoch 186/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9637\n",
            "Epoch 187/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0844 - accuracy: 0.9664\n",
            "Epoch 188/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0884 - accuracy: 0.9651\n",
            "Epoch 189/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.1198 - accuracy: 0.9637\n",
            "Epoch 190/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.1033 - accuracy: 0.9597\n",
            "Epoch 191/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0797 - accuracy: 0.9704\n",
            "Epoch 192/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0704 - accuracy: 0.9772\n",
            "Epoch 193/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0651 - accuracy: 0.9785\n",
            "Epoch 194/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0612 - accuracy: 0.9785\n",
            "Epoch 195/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0618 - accuracy: 0.9785\n",
            "Epoch 196/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0746 - accuracy: 0.9745\n",
            "Epoch 197/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0770 - accuracy: 0.9704\n",
            "Epoch 198/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0782 - accuracy: 0.9758\n",
            "Epoch 199/200\n",
            "15/15 [==============================] - 0s 2ms/step - loss: 0.0714 - accuracy: 0.9758\n",
            "Epoch 200/200\n",
            "15/15 [==============================] - 0s 3ms/step - loss: 0.0505 - accuracy: 0.9866\n",
            "24/24 [==============================] - 0s 1ms/step\n",
            "84/84 [==============================] - 0s 1ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      0.82      0.90      2617\n",
            "           1       0.08      0.69      0.15        62\n",
            "\n",
            "    accuracy                           0.82      2679\n",
            "   macro avg       0.54      0.76      0.52      2679\n",
            "weighted avg       0.97      0.82      0.88      2679\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###### Section(8) - New Metrics for Train_Test ######\n",
        "\n",
        "# Calculate Accuracy, Precision, Recall, F1-Score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "acc_train = accuracy_score(y_train_rus, y_pred_train)\n",
        "acc_test = accuracy_score(y_test, y_pred_test)\n",
        "pre_train = precision_score(y_train_rus, y_pred_train, average='macro')\n",
        "pre_test = precision_score(y_test, y_pred_test, average='macro')\n",
        "rec_train = recall_score(y_train_rus, y_pred_train, average='macro')\n",
        "rec_test = recall_score(y_test, y_pred_test, average ='macro')\n",
        "f1_train = f1_score(y_train_rus, y_pred_train, average='macro')\n",
        "f1_test = f1_score(y_test, y_pred_test, average='macro')\n",
        "# print the scores\n",
        "print('Accuracy scores of <ModelName> classifier are:', 'train: {:.2f}'.format(acc_train), 'and test: {:.2f}.'.format(acc_test))\n",
        "print('Precision scores of <ModelName> classifier are:', 'train: {:.2f}'.format(pre_train), 'and test: {:.2f}.'.format(pre_test))\n",
        "print('Recall scores of <ModelName> classifier are:', 'train: {:.2f}'.format(rec_train), 'and test: {:.2f}.'.format(rec_test))\n",
        "print('F1 scores of <ModelName> classifier are:', 'train: {:.2f}'.format(f1_train), 'and test: {:.2f}.'.format(f1_test))\n",
        "\n",
        "# Calculate TP, TN, FP, FN\n",
        "\n",
        "#print(logreg.score(X_test,y_test))\n",
        "#print(logreg.predict_proba(X_test))\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "#Calculatin TP,TN,FP,FN of training set\n",
        "tp_train, fp_train, fn_train, tn_train = confusion_matrix(y_train_rus, y_pred_train).ravel()\n",
        "print(tp_train, fp_train, fn_train, tn_train)\n",
        "\n",
        "#Calculatin TP,TN,FP,FN of test set\n",
        "tp_test, fp_test, fn_test, tn_test = confusion_matrix(y_test, y_pred_test, labels=[0, 1]).ravel()\n",
        "print(tp_test, fp_test, fn_test, tn_test)\n",
        "\n",
        "#Percentage Conditions of Models\n",
        "percantage_bankrupt = tn_test / np.count_nonzero(y_test == 1)\n",
        "percantage_non_bankrupt = tp_test / np.count_nonzero(y_test == 0)\n",
        "print(percantage_bankrupt)\n",
        "print(percantage_non_bankrupt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnyHDV0Tdm_J",
        "outputId": "b0f3a72b-6ede-45c9-c02e-a0136f0ffb5d"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy scores of <ModelName> classifier are: train: 0.98 and test: 0.82.\n",
            "Precision scores of <ModelName> classifier are: train: 0.98 and test: 0.54.\n",
            "Recall scores of <ModelName> classifier are: train: 0.98 and test: 0.76.\n",
            "F1 scores of <ModelName> classifier are: train: 0.98 and test: 0.52.\n",
            "553 5 7 179\n",
            "2150 467 19 43\n",
            "0.6935483870967742\n",
            "0.8215513947267864\n"
          ]
        }
      ]
    }
  ]
}
